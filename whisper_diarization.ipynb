{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRrSlonjYejw",
        "outputId": "656090de-bbc0-43cb-e1c6-beb37e55965e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'sudo' n'est pas reconnu en tant que commande interne\n",
            "ou externe, un programme ex�cutable ou un fichier de commandes.\n",
            "'sudo' n'est pas reconnu en tant que commande interne\n",
            "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
          ]
        }
      ],
      "source": [
        "!sudo apt update \n",
        "!sudo apt install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nr1Ct161Yr2J",
        "outputId": "b4622863-af6f-4003-c2a8-19a7d988f369"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to c:\\users\\user\\appdata\\local\\temp\\pip-req-build-qml0bqxa\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  ERROR: Error [WinError 2] Le fichier spécifié est introuvable while executing command git version\n",
            "ERROR: Cannot find command 'git' - do you have 'git' installed and in your PATH?\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKwWMfdCbPq4",
        "outputId": "6bc3d4b6-694b-45c9-f0db-bfb880307dc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.52.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.6.2.post1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (0.6.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q397BCV8s8YW",
        "outputId": "9f213ce8-37e0-44ef-92ff-248ed0cd72b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.0.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.26.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.15.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDk1CqqCY5zE",
        "outputId": "67749a9f-3a06-48fb-d7ff-75ca4235ca08"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['<|startoftranscript|><|en|><|transcribe|><|notimestamps|> Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "# load model and processor\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
        "model.config.forced_decoder_ids = None\n",
        "\n",
        "# load dummy dataset and read audio files\n",
        "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
        "sample = ds[0][\"audio\"]\n",
        "input_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features\n",
        "\n",
        "# generate token ids\n",
        "predicted_ids = model.generate(input_features)\n",
        "# decode token ids to text\n",
        "transcription1 = processor.batch_decode(predicted_ids, skip_special_tokens=False)\n",
        "\n",
        "transcription2 = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
        "transcription1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "BjUNbtHdtkmq"
      },
      "outputs": [],
      "source": [
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48X1OfsM2lDM",
        "outputId": "46341a10-4e5c-4f78-9ee4-9eb03cb59102"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transcription :  Good morning, ladies and gentlemen, dear colleagues. We're about to start our first session of the 40th Annual Meeting of Eschray. Welcome everybody again this morning. So we will start with our keynote lectures. We have two very exciting topics. There will be no questions since this is the keynote lecture. I hope you enjoy and then now I give the word to my colleague, Neil Slombalk, who will introduce the first speech.\n"
          ]
        }
      ],
      "source": [
        "import librosa\n",
        "\n",
        "# Charger le modèle et le processeur Whisper\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
        "\n",
        "# Charger le fichier audio (par exemple, \"audio_file.wav\")\n",
        "audio_file = \"data_conf_eshre_2024/0_audio.wav\"\n",
        "\n",
        "# Charger l'audio avec Librosa (remplacez 'path/to/audio.wav' par votre chemin)\n",
        "audio_input, sample_rate = librosa.load(audio_file, sr=None)  # sr=None pour garder la fréquence d'échantillonnage originale\n",
        "\n",
        "# Convertir à 16 kHz si nécessaire\n",
        "if sample_rate != 16000:\n",
        "    audio_input = librosa.resample(audio_input, orig_sr=sample_rate, target_sr=16000)\n",
        "    sample_rate = 16000\n",
        "\n",
        "# Prétraiter l'audio pour le modèle\n",
        "input_features = processor(audio_input, sampling_rate=sample_rate, return_tensors=\"pt\").input_features\n",
        "\n",
        "# Générer les transcriptions\n",
        "predicted_ids = model.generate(input_features)\n",
        "\n",
        "# Décoder les tokens en texte\n",
        "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
        "\n",
        "# Afficher la transcription\n",
        "print(\"Transcription :\", transcription[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "import string\n",
        "f = open(\"data_conf_eshre_2024/0_subtitles.txt\", \"r\")\n",
        "s = f.read().replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"\\r\", \" \")\n",
        "str1 = s\n",
        "cleaned_str1 = s.lower().translate(str.maketrans(\"\", \"\", string.punctuation))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "str2 = transcription[0]\n",
        "cleaned_str2 = transcription[0].lower().translate(str.maketrans(\"\", \"\", string.punctuation))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((424, 432), (412, 412))"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(len(str2),len(str1)),(len(cleaned_str2),len(cleaned_str2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Good morning, ladies and gentlemen, dear colleagues.  We're about to start our first session of the 40th  annual meeting of ASHRAE. Welcome,  everybody, again this morning.  So we will start with our keynote  lectures. We have two very exciting  topics. Um There  will be no questions since this is the  keynote lecture. I hope you enjoy, and  then now I give the word to my colleague,  Neil Slambalk, who will introduce the first  \""
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "str1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Distance de Levenshtein: 34\n"
          ]
        }
      ],
      "source": [
        "import Levenshtein\n",
        "\n",
        "\n",
        "levenshtein_distance = Levenshtein.distance(str1, str2)\n",
        "print(f\"Distance de Levenshtein: {levenshtein_distance}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    WER — Word Error Rate.\n",
        "    MER: Match Error Rate.\n",
        "    WIL: Word Information Lost.\n",
        "    CER: Character Error Rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' good morning ladies and gentlemen dear colleagues were about to start our first session of the 40th annual meeting of eschray welcome everybody again this morning so we will start with our keynote lectures we have two very exciting topics there will be no questions since this is the keynote lecture i hope you enjoy and then now i give the word to my colleague neil slombalk who will introduce the first speech'"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cleaned_str2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'good morning ladies and gentlemen dear colleagues  were about to start our first session of the 40th  annual meeting of ashrae welcome  everybody again this morning  so we will start with our keynote  lectures we have two very exciting  topics um there  will be no questions since this is the  keynote lecture i hope you enjoy and  then now i give the word to my colleague  neil slambalk who will introduce the first  '"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cleaned_str1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.4657534246575342,\n",
              " [('ashrae', 'eschray'),\n",
              "  ('um', 'there'),\n",
              "  ('there', 'will'),\n",
              "  ('will', 'be'),\n",
              "  ('be', 'no'),\n",
              "  ('no', 'questions'),\n",
              "  ('questions', 'since'),\n",
              "  ('since', 'this'),\n",
              "  ('this', 'is'),\n",
              "  ('is', 'the'),\n",
              "  ('the', 'keynote'),\n",
              "  ('keynote', 'lecture'),\n",
              "  ('lecture', 'i'),\n",
              "  ('i', 'hope'),\n",
              "  ('hope', 'you'),\n",
              "  ('you', 'enjoy'),\n",
              "  ('enjoy', 'and'),\n",
              "  ('and', 'then'),\n",
              "  ('then', 'now'),\n",
              "  ('now', 'i'),\n",
              "  ('i', 'give'),\n",
              "  ('give', 'the'),\n",
              "  ('the', 'word'),\n",
              "  ('word', 'to'),\n",
              "  ('to', 'my'),\n",
              "  ('my', 'colleague'),\n",
              "  ('colleague', 'neil'),\n",
              "  ('neil', 'slombalk'),\n",
              "  ('slambalk', 'who'),\n",
              "  ('who', 'will'),\n",
              "  ('will', 'introduce'),\n",
              "  ('introduce', 'the'),\n",
              "  ('the', 'first'),\n",
              "  ('first', 'speech')])"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def calculate_wer(reference, hypothesis):\n",
        "\tref_words = reference.split()\n",
        "\thyp_words = hypothesis.split()\n",
        "\t# Counting the number of substitutions, deletions, and insertions\n",
        "\tsubstitutions = sum(1 for ref, hyp in zip(ref_words, hyp_words) if ref != hyp)\n",
        "\tdeletions = len(ref_words) - len(hyp_words)\n",
        "\tinsertions = len(hyp_words) - len(ref_words)\n",
        "\t# Total number of words in the reference text\n",
        "\ttotal_words = len(ref_words)\n",
        "\t# Calculating the Word Error Rate (WER)\n",
        "\twer = (substitutions + deletions + insertions) / total_words\n",
        "\treturn wer,[(ref,hyp) for ref, hyp in zip(ref_words, hyp_words) if ref != hyp]\n",
        "#calculate_wer(str1,str2)\n",
        "calculate_wer(cleaned_str1,cleaned_str2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.1232876712328767, 0.0547945205479452)"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def wer_levenshtein(reference, hypothesis):\n",
        "    \"\"\"\n",
        "    Calcule le Word Error Rate (WER) entre une référence et une hypothèse.\n",
        "    \n",
        "    reference : la transcription de référence (chaîne de caractères)\n",
        "    hypothesis : la transcription prédite par le modèle whisper (chaîne de caractères)\n",
        "    \"\"\"\n",
        "    ref_words = reference.split()\n",
        "    hyp_words = hypothesis.split()\n",
        "    \n",
        "    # Créer la matrice pour l'algorithme de Levenshtein\n",
        "    d = np.zeros((len(ref_words) + 1, len(hyp_words) + 1), dtype=np.uint8)\n",
        "    \n",
        "    # Initialisation de la première ligne et colonne\n",
        "    for i in range(len(ref_words) + 1):\n",
        "        d[i][0] = i\n",
        "    for j in range(len(hyp_words) + 1):\n",
        "        d[0][j] = j\n",
        "    \n",
        "    # Remplir la matrice de distance\n",
        "    for i in range(1, len(ref_words) + 1):\n",
        "        for j in range(1, len(hyp_words) + 1):\n",
        "            if ref_words[i - 1] == hyp_words[j - 1]:\n",
        "                cost = 0\n",
        "            else:\n",
        "                cost = 1\n",
        "            d[i][j] = min(d[i - 1][j] + 1,  # Suppression\n",
        "                          d[i][j - 1] + 1,  # Insertion\n",
        "                          d[i - 1][j - 1] + cost)  # Substitution\n",
        "    \n",
        "    # Le WER est le ratio entre le nombre d'erreurs et le nombre de mots de référence\n",
        "    wer_result = d[len(ref_words)][len(hyp_words)] / float(len(ref_words))\n",
        "    return wer_result\n",
        "wer_levenshtein(str1,str2),wer_levenshtein(cleaned_str1,cleaned_str2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchmetrics.text import MatchErrorRate,WordErrorRate,WordInfoLost,CharErrorRate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(0.0548), tensor(0.1233))"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mer = MatchErrorRate() \n",
        "mer(cleaned_str1,cleaned_str2),mer(str1,str2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(0.0548), tensor(0.1233))"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wer = WordErrorRate() \n",
        "wer(cleaned_str1,cleaned_str2),wer(str1,str2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(0.1066), tensor(0.2314))"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wil = WordInfoLost() \n",
        "wil(cleaned_str1,cleaned_str2),wil(str1,str2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(0.0583), tensor(0.0802))"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cer = CharErrorRate() \n",
        "cer(cleaned_str1,cleaned_str2),cer(str1,str2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers import BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 117496504320.0\n"
          ]
        }
      ],
      "source": [
        "refs = []\n",
        "cleaned_refs = []\n",
        "predicts = []\n",
        "cleaned_predicts = []\n",
        "input_ids = []\n",
        "for i in range(11):\n",
        "    f = open(\"data_conf_eshre_2024/\"+str(i)+\"_subtitles.txt\", \"r\")\n",
        "    s = f.read().replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"\\r\", \" \")\n",
        "    refs.append(s) \n",
        "    cleaned_refs.append( s.lower().translate(str.maketrans(\"\", \"\", string.punctuation)))\n",
        "    \n",
        "    # Charger le fichier audio (par exemple, \"audio_file.wav\")\n",
        "    audio_file = \"data_conf_eshre_2024/\"+str(i)+\"_audio.wav\"\n",
        "\n",
        "    # Charger l'audio avec Librosa (remplacez 'path/to/audio.wav' par votre chemin)\n",
        "    audio_input, sample_rate = librosa.load(audio_file, sr=None)  # sr=None pour garder la fréquence d'échantillonnage originale\n",
        "\n",
        "    # Convertir à 16 kHz si nécessaire\n",
        "    if sample_rate != 16000:\n",
        "        audio_input = librosa.resample(audio_input, orig_sr=sample_rate, target_sr=16000)\n",
        "        sample_rate = 16000\n",
        "\n",
        "    # Prétraiter l'audio pour le modèle\n",
        "    input_features = processor(audio_input, sampling_rate=sample_rate, return_tensors=\"pt\").input_features\n",
        "\n",
        "    # Générer les transcriptions\n",
        "    predicted_ids = model.generate(input_features)\n",
        "    input_ids.append(predicted_ids)\n",
        "    # Décoder les tokens en texte\n",
        "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
        "    predicts.append(transcription[0])\n",
        "    # Afficher la transcription\n",
        "    #print(\"Transcription :\", transcription[0])\n",
        "    \n",
        "# Créer une instance de BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# Utiliser le tokenizer BERT pour tokeniser les textes\n",
        "tokenized_refs = [tokenizer(ref, return_tensors=\"pt\", padding=True, truncation=True).input_ids for ref in refs]\n",
        "tokenized_preds = [tokenizer(pred, return_tensors=\"pt\", padding=True, truncation=True).input_ids for pred in predicts]\n",
        "# Assurez-vous que les tenseurs sont de la même taille\n",
        "max_length = max(len(x[0]) for x in tokenized_refs + tokenized_preds)\n",
        "\n",
        "# Padding des tenseurs\n",
        "tokenized_refs = [torch.nn.functional.pad(t, (0, max_length - t.size(1))) for t in tokenized_refs]\n",
        "tokenized_preds = [torch.nn.functional.pad(t, (0, max_length - t.size(1))) for t in tokenized_preds]\n",
        "\n",
        "# Conversion en tenseurs\n",
        "refs_tensor = torch.cat(tokenized_refs).float()\n",
        "preds_tensor = torch.cat(tokenized_preds).float()\n",
        "\n",
        "# Calcul de la perte\n",
        "loss_fn = CrossEntropyLoss()\n",
        "loss = loss_fn(preds_tensor.view(-1), refs_tensor.view(-1))\n",
        "\n",
        "print(f\"Loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perte pour l'échantillon 0 : 0.012054750695824623\n",
            "Perte pour l'échantillon 1 : 0.0014057350344955921\n",
            "Perte pour l'échantillon 2 : 0.001003621844574809\n",
            "Perte pour l'échantillon 3 : 0.0012429757043719292\n",
            "Perte pour l'échantillon 4 : 0.04486268013715744\n",
            "Perte pour l'échantillon 5 : 0.0013033371651545167\n",
            "Perte pour l'échantillon 6 : 0.0015291206073015928\n",
            "Perte pour l'échantillon 7 : 0.001194818178191781\n",
            "Perte pour l'échantillon 8 : 0.08920136839151382\n",
            "Perte pour l'échantillon 9 : 0.0020029034931212664\n",
            "Perte pour l'échantillon 10 : 0.0014694276032969356\n",
            "Perte totale : 0.1572707388550043\n",
            "Perte moyenne : 0.014297339895909483\n"
          ]
        }
      ],
      "source": [
        "# Initialiser la fonction de perte Cross-Entropy\n",
        "loss_fn = CrossEntropyLoss()\n",
        "total_loss = 0.0  # Variable pour stocker la perte totale\n",
        "num_samples = 11  # Nombre total d'échantillons\n",
        "refs = []  # Pour stocker les transcriptions de référence\n",
        "input_ids = []  # Pour stocker les transcriptions générées\n",
        "predicts = []  # Pour stocker les textes prédits\n",
        "\n",
        "# Initialiser le processeur Whisper (tokenizer + feature extractor)\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
        "\n",
        "for i in range(11):\n",
        "    # Charger les sous-titres\n",
        "    with open(\"data_conf_eshre_2024/\"+str(i)+\"_subtitles.txt\", \"r\") as f:\n",
        "        s = f.read().replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"\\r\", \" \")\n",
        "    refs.append(s.lower().translate(str.maketrans(\"\", \"\", string.punctuation))) \n",
        "    \n",
        "    # Charger le fichier audio\n",
        "    audio_file = \"data_conf_eshre_2024/\" + str(i) + \"_audio.wav\"\n",
        "    audio_input, sample_rate = librosa.load(audio_file, sr=None)\n",
        "\n",
        "    # Convertir à 16 kHz si nécessaire\n",
        "    if sample_rate != 16000:\n",
        "        audio_input = librosa.resample(audio_input, orig_sr=sample_rate, target_sr=16000)\n",
        "        sample_rate = 16000\n",
        "\n",
        "    # Prétraiter l'audio pour le modèle (audio -> features)\n",
        "    input_features = processor(audio_input, sampling_rate=sample_rate, return_tensors=\"pt\").input_features\n",
        "    \n",
        "    # Tokenisation des sous-titres de référence (texte -> tokens)\n",
        "    target_tokens = processor.tokenizer(refs[i], return_tensors=\"pt\").input_ids\n",
        "    # Obtenir les logits du modèle en fournissant les decoder_input_ids\n",
        "    outputs = model(input_features=input_features, decoder_input_ids=target_tokens)\n",
        "\n",
        "    # Les logits sont les résultats bruts pour la génération de texte\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # Calcul de la perte (entropie croisée) entre les logits et les tokens cibles\n",
        "    loss = loss_fn(logits.view(-1, logits.size(-1)), target_tokens.view(-1))\n",
        "    # Ajouter la perte à la perte totale\n",
        "    total_loss += loss.item()\n",
        "    # Afficher la perte pour cet échantillon\n",
        "    print(f\"Perte pour l'échantillon {i} : {loss.item()}\")\n",
        "    \n",
        "# Calcul de la perte moyenne\n",
        "average_loss = total_loss / num_samples\n",
        "print(f\"Perte totale : {total_loss}\")\n",
        "print(f\"Perte moyenne : {average_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# train whisper model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os \n",
        "import pandas as pd\n",
        "audio_dir = \"data_conf_eshre_2024\"  # Répertoire contenant les fichiers audio\n",
        "\n",
        "audio_files = [audio_dir+'/'+ f for f in os.listdir(audio_dir) if f.endswith('.wav')]\n",
        "audio_texts = [f for f in os.listdir(audio_dir) if f.endswith('.txt')]\n",
        "\n",
        "texts = []\n",
        "input_features = []\n",
        "audios = []\n",
        "for idx in range(11):\n",
        "    audio_file = audio_files[idx]\n",
        "    audio_path = audio_file\n",
        "    text_file = audio_texts[idx]\n",
        "    text_path = os.path.join(audio_dir, text_file)\n",
        "\n",
        "    # Charger l'audio\n",
        "    audio_input, sample_rate = librosa.load(audio_path, sr=None)\n",
        "\n",
        "    # Convertir à 16 kHz si nécessaire\n",
        "    if sample_rate != 16000:\n",
        "        audio_input = librosa.resample(audio_input, orig_sr=sample_rate, target_sr=16000)\n",
        "        sample_rate = 16000\n",
        "    audios.append(audio_input)\n",
        "    # Charger les sous-titres\n",
        "    with open(text_path, 'r') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # Nettoyer le texte\n",
        "    texts.append(text.lower().translate(str.maketrans(\"\", \"\", string.punctuation)).replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"\\r\", \" \"))\n",
        "    # Prétraiter l'audio pour le modèle\n",
        "    input_features.append(processor(audio_input, sampling_rate=sample_rate, return_tensors=\"pt\", padding=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>files</th>\n",
              "      <th>audios</th>\n",
              "      <th>input_features</th>\n",
              "      <th>texts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>data_conf_eshre_2024/0_audio.wav</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[input_features]</td>\n",
              "      <td>good morning ladies and gentlemen dear colleag...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>data_conf_eshre_2024/10_audio.wav</td>\n",
              "      <td>[-0.10670296, -0.22843748, -0.22037433, -0.172...</td>\n",
              "      <td>[input_features]</td>\n",
              "      <td>see or disorders to  the opposite there are ma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>data_conf_eshre_2024/1_audio.wav</td>\n",
              "      <td>[-0.061012916, -0.10953534, -0.09543714, -0.10...</td>\n",
              "      <td>[input_features]</td>\n",
              "      <td>speaker thank you very much and once  more a h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>data_conf_eshre_2024/2_audio.wav</td>\n",
              "      <td>[0.024932653, 0.04252725, 0.008827795, -0.0068...</td>\n",
              "      <td>[input_features]</td>\n",
              "      <td>usability on the internet and  the selection f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>data_conf_eshre_2024/3_audio.wav</td>\n",
              "      <td>[0.028431522, 0.035124354, 0.02412346, 0.02647...</td>\n",
              "      <td>[input_features]</td>\n",
              "      <td>epidemiology statistics unit of the  universit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>data_conf_eshre_2024/4_audio.wav</td>\n",
              "      <td>[0.01154748, -0.04733802, -0.10141198, -0.1160...</td>\n",
              "      <td>[input_features]</td>\n",
              "      <td>thanks very much for the introduction  the cha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>data_conf_eshre_2024/5_audio.wav</td>\n",
              "      <td>[-0.0899973, 0.023822498, 0.08860321, -0.10559...</td>\n",
              "      <td>[input_features]</td>\n",
              "      <td>as fertility professionals we are proud  to su...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>data_conf_eshre_2024/6_audio.wav</td>\n",
              "      <td>[-0.13563028, -0.29694942, -0.36510634, -0.424...</td>\n",
              "      <td>[input_features]</td>\n",
              "      <td>pregnancy miscarriage pregnancy  complications...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>data_conf_eshre_2024/7_audio.wav</td>\n",
              "      <td>[0.0118788555, 0.01665976, 0.016094869, 0.0142...</td>\n",
              "      <td>[input_features]</td>\n",
              "      <td>secondly it typically ranges from 20 to  28 we...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>data_conf_eshre_2024/8_audio.wav</td>\n",
              "      <td>[0.0080893, 0.020648647, 0.038412217, 0.062824...</td>\n",
              "      <td>[input_features]</td>\n",
              "      <td>emotional impact such as grief  guilt sadness ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>data_conf_eshre_2024/9_audio.wav</td>\n",
              "      <td>[-0.068803705, -0.013883192, 0.030553743, -0.0...</td>\n",
              "      <td>[input_features]</td>\n",
              "      <td>fetus still viable surrendered  miscarriage af...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                files  \\\n",
              "0    data_conf_eshre_2024/0_audio.wav   \n",
              "1   data_conf_eshre_2024/10_audio.wav   \n",
              "2    data_conf_eshre_2024/1_audio.wav   \n",
              "3    data_conf_eshre_2024/2_audio.wav   \n",
              "4    data_conf_eshre_2024/3_audio.wav   \n",
              "5    data_conf_eshre_2024/4_audio.wav   \n",
              "6    data_conf_eshre_2024/5_audio.wav   \n",
              "7    data_conf_eshre_2024/6_audio.wav   \n",
              "8    data_conf_eshre_2024/7_audio.wav   \n",
              "9    data_conf_eshre_2024/8_audio.wav   \n",
              "10   data_conf_eshre_2024/9_audio.wav   \n",
              "\n",
              "                                               audios    input_features  \\\n",
              "0   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  [input_features]   \n",
              "1   [-0.10670296, -0.22843748, -0.22037433, -0.172...  [input_features]   \n",
              "2   [-0.061012916, -0.10953534, -0.09543714, -0.10...  [input_features]   \n",
              "3   [0.024932653, 0.04252725, 0.008827795, -0.0068...  [input_features]   \n",
              "4   [0.028431522, 0.035124354, 0.02412346, 0.02647...  [input_features]   \n",
              "5   [0.01154748, -0.04733802, -0.10141198, -0.1160...  [input_features]   \n",
              "6   [-0.0899973, 0.023822498, 0.08860321, -0.10559...  [input_features]   \n",
              "7   [-0.13563028, -0.29694942, -0.36510634, -0.424...  [input_features]   \n",
              "8   [0.0118788555, 0.01665976, 0.016094869, 0.0142...  [input_features]   \n",
              "9   [0.0080893, 0.020648647, 0.038412217, 0.062824...  [input_features]   \n",
              "10  [-0.068803705, -0.013883192, 0.030553743, -0.0...  [input_features]   \n",
              "\n",
              "                                                texts  \n",
              "0   good morning ladies and gentlemen dear colleag...  \n",
              "1   see or disorders to  the opposite there are ma...  \n",
              "2   speaker thank you very much and once  more a h...  \n",
              "3   usability on the internet and  the selection f...  \n",
              "4   epidemiology statistics unit of the  universit...  \n",
              "5   thanks very much for the introduction  the cha...  \n",
              "6   as fertility professionals we are proud  to su...  \n",
              "7   pregnancy miscarriage pregnancy  complications...  \n",
              "8   secondly it typically ranges from 20 to  28 we...  \n",
              "9   emotional impact such as grief  guilt sadness ...  \n",
              "10  fetus still viable surrendered  miscarriage af...  "
            ]
          },
          "execution_count": 146,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "d =  {'files':audio_files,'audios':audios,'input_features':input_features ,'texts':texts }\n",
        "df = pd.DataFrame(d)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Diviser le DataFrame en ensembles d'entraînement et de test (80/20)\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Loss: 2.8629250191152096\n",
            "Epoch 2/5, Loss: 0.14535404392518103\n",
            "Epoch 3/5, Loss: 0.013249049603473395\n",
            "Epoch 4/5, Loss: 0.003872024841257371\n",
            "Epoch 5/5, Loss: 0.0019227275770390406\n"
          ]
        }
      ],
      "source": [
        "from torch import  optim\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Mettre le modèle en mode entraînement\n",
        "model.train()\n",
        "\n",
        "# Boucle d'entraînement\n",
        "num_epochs = 5  # Ajuste le nombre d'époques\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for index, row in train_df.iterrows():\n",
        "        audio_path = row['files']\n",
        "        transcription = row['texts']\n",
        "\n",
        "        # Charger l'audio\n",
        "        audio_input, sample_rate = librosa.load(audio_path, sr=None)\n",
        "\n",
        "        # Convertir à 16 kHz si nécessaire\n",
        "        if sample_rate != 16000:\n",
        "            audio_input = librosa.resample(audio_input, orig_sr=sample_rate, target_sr=16000)\n",
        "\n",
        "        # Prétraiter l'audio pour le modèle\n",
        "        input_features = processor(audio_input, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "        # Nettoyer et tokeniser la transcription\n",
        "        cleaned_transcription = transcription.lower().translate(str.maketrans(\"\", \"\", string.punctuation)).replace(\"\\n\", \" \")\n",
        "        target_tokens = processor.tokenizer(cleaned_transcription, return_tensors=\"pt\", padding=True).input_ids\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Obtenir les logits du modèle\n",
        "        outputs = model(input_features=input_features['input_features'], decoder_input_ids=target_tokens)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Calculer la perte\n",
        "        loss = loss_fn(logits.view(-1, logits.size(-1)), target_tokens.view(-1))\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.04486268013715744\n",
            "Test Loss: 0.012054750695824623\n",
            "Test Loss: 0.08920136839151382\n",
            "Test Loss: 0.0487062664081653\n"
          ]
        }
      ],
      "source": [
        "# Évaluation sur l'ensemble de test\n",
        "model.eval()  # Mettre le modèle en mode évaluation\n",
        "total_test_loss = 0\n",
        "with torch.no_grad():  # Pas de calcul de gradient pendant l'évaluation\n",
        "    for index, row in test_df.iterrows():\n",
        "        audio_path = row['files']\n",
        "        transcription = row['texts']\n",
        "\n",
        "        # Charger l'audio\n",
        "        audio_input, sample_rate = librosa.load(audio_path, sr=None)\n",
        "\n",
        "        # Convertir à 16 kHz si nécessaire\n",
        "        if sample_rate != 16000:\n",
        "            audio_input = librosa.resample(audio_input, orig_sr=sample_rate, target_sr=16000)\n",
        "\n",
        "        # Prétraiter l'audio pour le modèle\n",
        "        input_features = processor(audio_input, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "        # Nettoyer et tokeniser la transcription\n",
        "        cleaned_transcription = transcription.lower().translate(str.maketrans(\"\", \"\", string.punctuation)).replace(\"\\n\", \" \")\n",
        "        target_tokens = processor.tokenizer(cleaned_transcription, return_tensors=\"pt\", padding=True).input_ids\n",
        "\n",
        "        # Obtenir les logits du modèle\n",
        "        outputs = model(input_features=input_features['input_features'], decoder_input_ids=target_tokens)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Calculer la perte\n",
        "        loss = loss_fn(logits.view(-1, logits.size(-1)), target_tokens.view(-1))\n",
        "        total_test_loss += loss.item()\n",
        "        print(f\"Test Loss: {loss.item() }\")\n",
        " \n",
        "print(f\"Total Loss: {total_test_loss / len(test_df)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Train Loss: 0.0012461995938792825, Val Loss: 0.000798483844846487\n",
            "Epoch 2/5, Train Loss: 0.0009611300580824415, Val Loss: 0.0006683822721242905\n",
            "Epoch 3/5, Train Loss: 0.0007874409105473509, Val Loss: 0.0005786966648884118\n",
            "Epoch 4/5, Train Loss: 0.0006733149542318037, Val Loss: 0.0005130088538862765\n",
            "Epoch 5/5, Train Loss: 0.0005932027076293404, Val Loss: 0.0004627755261026323\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
        "# Boucle d'entraînement\n",
        "num_epochs = 5\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for index, row in train_df.iterrows():\n",
        "        \n",
        "        audio_path = row['files']\n",
        "        transcription = row['texts']\n",
        "\n",
        "        # Charger l'audio\n",
        "        audio_input, sample_rate = librosa.load(audio_path, sr=None)\n",
        "\n",
        "        # Convertir à 16 kHz si nécessaire\n",
        "        if sample_rate != 16000:\n",
        "            audio_input = librosa.resample(audio_input, orig_sr=sample_rate, target_sr=16000)\n",
        "\n",
        "        # Prétraiter l'audio pour le modèle\n",
        "        input_features = processor(audio_input, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "        # Nettoyer et tokeniser la transcription\n",
        "        cleaned_transcription = transcription.lower().translate(str.maketrans(\"\", \"\", string.punctuation)).replace(\"\\n\", \" \")\n",
        "        target_tokens = processor.tokenizer(cleaned_transcription, return_tensors=\"pt\", padding=True).input_ids\n",
        "\n",
        "        # Backpropagation et mise à jour des poids\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_features=input_features['input_features'], decoder_input_ids=target_tokens)\n",
        "        logits = outputs.logits\n",
        "        loss = loss_fn(logits.view(-1, logits.size(-1)), target_tokens.view(-1))\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculer la perte d'entraînement moyenne\n",
        "    train_losses.append(total_loss / len(train_df))\n",
        "    \n",
        "    # Évaluation sur l'ensemble de validation\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for index, row in val_df.iterrows():\n",
        "            audio_path = row['files']\n",
        "            transcription = row['texts']\n",
        "            # ... (charge l'audio, prétraitement, etc.)\n",
        "\n",
        "            outputs = model(input_features=input_features['input_features'], decoder_input_ids=target_tokens)\n",
        "            logits = outputs.logits\n",
        "            loss = loss_fn(logits.view(-1, logits.size(-1)), target_tokens.view(-1))\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    val_losses.append(total_val_loss / len(val_df))\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_losses[-1]}, Val Loss: {val_losses[-1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.title('Learning Curve')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('emotional impact such as grief  guilt sadness or even fear of future  pregnancies  miscarriage often does not occur  suddenly a condition called  threatened miscarriage precedes  inevitable miscarriage often  and threatened miscarriage is  mischaracterized by vaginal bleeding with  or without abnormal pain and in these  cases the cervix remains closed  ', 'good morning ladies and gentlemen dear colleagues  were about to start our first session of the 40th  annual meeting of ashrae welcome  everybody again this morning  so we will start with our keynote  lectures we have two very exciting  topics um there  will be no questions since this is the  keynote lecture i hope you enjoy and  then now i give the word to my colleague  neil slambalk who will introduce the first  ', 'fetus still viable surrendered  miscarriage affects around 25 of all  clinical pregnancies and among those  25 of them will develop into inevitable  miscarriage  speaking of the etiology of miscarriage  there are actually many causes um  here just lists some common categories  and its not exhaustive some of the  causes are modifiable or treatable such  as progesterone deficiency  ', 'speaker thank you very much and once  more a heartily welcome to this  outstanding and sizable meeting the 40th  ashrae annual meeting it  is my pleasure to introduce to  you the human reproduction keynote  lecture traditionally that is  based on a paper that  within six months after its publication  yielded very high rates of  downloads and use  ')\n",
            "('epidemiology statistics unit of the  university of new south wales  hes also an australian national health  and medical research council fellow wen  tao has extensive  experience with national health and  medical research council  and evidence synthesis his work  focuses on reproductive health and  obstetrics aiming to improve clinical  practices through reliable evidence and  wen tao the floor is yours  ', 'secondly it typically ranges from 20 to  28 weeks and this variation does  impose challenges in communications  according to the latest actually  guideline on the management of  recurrent pregnancy loss the definition  of miscarriage was those pregnancy loss  that happened below 24 weeks  it is important to recognize that  miscarriage is not just a physical  condition oftentimes it has  ', 'pregnancy miscarriage pregnancy  complications stillbirth preterm  birth neonatal disease or even death  today ill be focusing on one of the  major challenges which is miscarriage  miscarriage is defined as the loose of  the fetus before it is viable  this sounds pretty straightforward  but the gestational age threshold for  defining what makes a miscarriage  very significant  ', 'see or disorders to  the opposite there are many factors that  are not modifiable as is for  now such as those caused by chromosomal  abnormalities or genetic mutations  although we could work really hard to  find the causes of a case of  miscarriage in many cases we wouldnt be  able to do so and placing these cases  into the no clear cause category  as just mentioned  ')\n",
            "('usability on the internet and  the selection for today  is about progesterone  for women with threatened miscarriage  the stop trial a placebocontrolled  randomized clinical trial the first  author was lucas maclinden  the paper will be presented by the senior  author wentao li from  melbourne wentao is a clinical  epidemiologist at the national perinatal  ', 'thanks very much for the introduction  the chair ladies and gentlemen a very  good morning to  start off i want to thank the human  reduction editorial team and the ashrae  organizing committee for inviting me it  is truly an honor to be here and talk  about a topic that is so important to  many expectant parents  this is my cufflower interest and some  extra disclosures  ', 'as fertility professionals we are proud  to support couples becoming parents  we know for some people this does not  always come easily and there are many  challenges on the road to have a healthy  baby while  infertility often takes the center stage  within discussions in this community  but at the same time we need to be aware  that there are other challenges once  pregnancy has established um to  name a few this could include ectopic  ')\n"
          ]
        }
      ],
      "source": [
        "for batch in dataloader:\n",
        "        input_features, target_tokens = batch\n",
        "        print(target_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_4764\\671443687.py:21: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_input, sample_rate = librosa.load(audio_path, sr=None)\n",
            "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "ename": "NoBackendError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\audio.py:176\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m sf\u001b[38;5;241m.\u001b[39mSoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\audio.py:209\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;66;03m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context \u001b[38;5;28;01mas\u001b[39;00m sf_desc:\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\soundfile.py:658\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[0;32m    657\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[1;32m--> 658\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\soundfile.py:1216\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[1;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[0;32m   1215\u001b[0m     err \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error(file_ptr)\n\u001b[1;32m-> 1216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[0;32m   1218\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n",
            "\u001b[1;31mLibsndfileError\u001b[0m: Error opening 'data_conf_eshre_2024\\\\1_subtitles.txt': Format not recognised.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mNoBackendError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[96], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "Cell \u001b[1;32mIn[90], line 21\u001b[0m, in \u001b[0;36mAudioTextDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     18\u001b[0m subtitle_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubtitles_dir, subtitle_file)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Charger l'audio\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m audio_input, sample_rate \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Convertir à 16 kHz si nécessaire\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_rate \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m16000\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\audio.py:184\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPurePath)):\n\u001b[0;32m    181\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    183\u001b[0m     )\n\u001b[1;32m--> 184\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__audioread_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\util\\decorators.py:59\u001b[0m, in \u001b[0;36mdeprecated.<locals>.__wrapper\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Warn the user, and then proceed.\"\"\"\u001b[39;00m\n\u001b[0;32m     51\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mDeprecated as of librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mIt will be removed in librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# Would be 2, but the decorator adds a level\u001b[39;00m\n\u001b[0;32m     58\u001b[0m )\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\audio.py:240\u001b[0m, in \u001b[0;36m__audioread_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    237\u001b[0m     reader \u001b[38;5;241m=\u001b[39m path\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# If the input was not an audioread object, try to open it\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[43maudioread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m reader \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[0;32m    243\u001b[0m     sr_native \u001b[38;5;241m=\u001b[39m input_file\u001b[38;5;241m.\u001b[39msamplerate\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\audioread\\__init__.py:132\u001b[0m, in \u001b[0;36maudio_open\u001b[1;34m(path, backends)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# All backends failed!\u001b[39;00m\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m NoBackendError()\n",
            "\u001b[1;31mNoBackendError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for epoch in range(20):\n",
        "    for batch in dataloader:\n",
        "        input_features, target_tokens = batch\n",
        "        print(batch)\n",
        "        \n",
        "\n",
        "        # Passer par le modèle et calculer les logits\n",
        "        outputs = model(input_features=input_features, decoder_input_ids=target_tokens)\n",
        "\n",
        "        # Obtenir les logits\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Calculer la perte\n",
        "        loss_fn = torch.nn.CrossEntropyLoss(ignore_index=processor.tokenizer.pad_token_id)\n",
        "        loss = loss_fn(logits.view(-1, logits.size(-1)), target_tokens.view(-1))\n",
        "\n",
        "        # Optimiser les poids\n",
        "        optimizer.zero_grad()  # Remettre à zéro les gradients\n",
        "        loss.backward()  # Calculer les gradients\n",
        "        optimizer.step()  # Mettre à jour les poids\n",
        "\n",
        "        # Afficher la perte\n",
        "        print(f\"Epoch [{epoch+1}/{20}], Loss: {loss.item():.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Diarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob, os, subprocess\n",
        "import torch\n",
        "import pyannote.audio\n",
        "from sklearn.cluster import AgglomerativeClustering,DBSCAN\n",
        "from pyannote.audio import Audio\n",
        "from pyannote.core import Segment\n",
        "import wave\n",
        "import contextlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\speechbrain\\utils\\checkpoints.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(path, map_location=device)\n",
            "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\speechbrain\\processing\\features.py:1311: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  stats = torch.load(path, map_location=device)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import subprocess\n",
        "from sklearn.cluster import DBSCAN\n",
        "from scipy.spatial.distance import cosine\n",
        "from pyannote.audio import Audio\n",
        "from pyannote.core import Segment\n",
        "import wave, contextlib\n",
        "\n",
        "embedding_model = PretrainedSpeakerEmbedding(\"speechbrain/spkrec-ecapa-voxceleb\")\n",
        "audio = Audio()  # Instantiate the Audio class\n",
        "\n",
        "def is_same_speaker(embedding1, embedding2, threshold=0.5):\n",
        "    \"\"\"Compare two embeddings using cosine distance\"\"\"\n",
        "    return cosine(embedding1, embedding2) < threshold\n",
        "\n",
        "def extract_speakers(model, path):\n",
        "    \"\"\"Do diarization with speaker names\"\"\"\n",
        "    \n",
        "    mono = 'mono.wav'\n",
        "    cmd = f'ffmpeg -i {path} -y -ac 1 {mono}'\n",
        "    subprocess.check_output(cmd, shell=True)\n",
        "    \n",
        "    result = model.transcribe(mono)\n",
        "    segments = result[\"segments\"]\n",
        "    \n",
        "    with contextlib.closing(wave.open(mono, 'r')) as f:\n",
        "        frames = f.getnframes()\n",
        "        rate = f.getframerate()\n",
        "        duration = frames / float(rate)\n",
        "        \n",
        "    def segment_embedding(segment):\n",
        "        start = segment[\"start\"]\n",
        "        end = min(duration, segment[\"end\"])\n",
        "        clip = Segment(start, end)\n",
        "        \n",
        "        # Load the audio segment using Audio.crop\n",
        "        waveform, sample_rate = Audio().crop(mono, clip)\n",
        "        return embedding_model(waveform[None])\n",
        "\n",
        "    # Generate embeddings for each segment\n",
        "    embeddings = np.zeros(shape=(len(segments), 192))\n",
        "    for i, segment in enumerate(segments):\n",
        "        embeddings[i] = segment_embedding(segment)\n",
        "\n",
        "    # Normalize embeddings\n",
        "    embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "    \n",
        "    # Apply DBSCAN clustering\n",
        "    clustering = DBSCAN(eps=0.5, min_samples=2).fit(embeddings)\n",
        "    labels = clustering.labels_\n",
        "\n",
        "    for i in range(len(segments)):\n",
        "        segments[i][\"speaker\"] = 'SPEAKER ' + str(labels[i] + 1)\n",
        "    \n",
        "    # Optional: Further refinement using cosine distance\n",
        "    for i in range(1, len(segments)):\n",
        "        if not is_same_speaker(embeddings[i], embeddings[i - 1], threshold=0.5):\n",
        "            segments[i][\"speaker\"] = 'SPEAKER ' + str(labels[i] + 2)\n",
        "\n",
        "    return segments\n",
        "\n",
        "def write_segments(segments, outfile):\n",
        "    \"\"\"Write out segments to file\"\"\"\n",
        "    \n",
        "    def time(secs):\n",
        "        return datetime.timedelta(seconds=round(secs))\n",
        "    \n",
        "    with open(outfile, \"w\") as f:    \n",
        "        for i, segment in enumerate(segments):\n",
        "            if i == 0 or segments[i - 1][\"speaker\"] != segment[\"speaker\"]:\n",
        "                f.write(\"\\n\" + segment[\"speaker\"] + ' ' + str(time(segment[\"start\"])) + '\\n')\n",
        "            f.write(segment[\"text\"][1:] + ' ')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\whisper\\__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " I'd like to be honest, thank you for seeing us today. Of course Laura, it's great to meet you both. How are you feeling about everything today? Well we are excited but also nervous. It feels like there is so much to think about. Yeah we've been reading up on things but it's a lot to take in. We're just trying to make sense of it all. Well look that's really understandable. There are many best parenthood and it sometimes might seem overwhelming or complicated and we will work together to find the best one for you guys. Laura, since you are planning to carry the baby, could you tell me a bit more about your menstrual cycle? Is it regular? Yeah please, could you elaborate on that? Yes, it's mostly regular. My cycles are around 28 days but I've been noticing more painful periods lately so I'm not sure if that's something to worry about or not. Oh brilliant. This is really good to mention. Painful periods can sometimes be linked to conditions like endometriosis when the tissue of your uterus travels into your pelvis, other areas and it might actually impact fertility. Have you ever been evaluated or seen by a doctor about this painful periods before? No, I never had any tests. I just thought it was normal. We've been wondering if it could be a problem but we weren't sure if it was worth bringing up. Well, it's always really worth mentioning so thank you for bringing it up. We'll start by scheduling some basic tests including an ultramar sound to check for any potential issues. This will help us to make sure there is nothing that could complicate things. Does that sound okay to you guys? Yeah, sounds good. Definitely. We want to be sure we're not missing anything. Great. Now let's talk about your family history. Have you, have there been any fertility related issues in your family like early men oppose or conditions such as polycystic ovarian syndrome that you are aware of? Actually, yes. My mom went through menopause early so around 42 I think. So I've been a bit worried about that too. Yeah, that's good. Early menopause and family members can sometimes indicate lower ovarian reserve which means there might be fewer eggs available in the ovary and fertility could actually decrease earlier. It doesn't mean that will definitely happen to you but we will do some tests to check your ovarian reserve including a hormone called AMH and also along with the ultrasound and the antrophotocal sound we will get some help as well. So does that mean we need to hurry with the treatment? Not necessarily but it's better to have this information now so we can make informed decisions. If the test show lower ovarian reserve we might recommend moving a bit faster or even consider egg freezing if you are open to it but we will wait for the results before jumping into conclusions. Okay, that makes sense. I hadn't thought about egg freezing but it's good to know that it's an option. Great. So what's the option? Based on what you've shared I'd recommend starting with an intrauterion insemination or what's called IOUI. It's less invasive, let's invasive treatment story and it's often successful for same sex couples using donor sperm. Does that sound like something you are open to? Yes, we were open to start with something less intense than IVF so IOUI sounds like a good first step. I agree. We've already talked to a friend who is willing to be our donor. Would that work for us? That's wonderful. Having an on donor can make the process more personal. We'll need to arrange some health secreting for the donor to ensure everything is in order. How do you both feel about using a fresh sperm sample versus frozen one? Well, what would you recommend? Well if your donor is local and willing to provide a sample in the clinic, a fresh sample might slightly improve your chances in having a normal pregnancy and a baby but frozen sample, it just offers more flexibility when it comes to the timing of the IOUI. So what's your preference? I think fresh would be best. It feels like more natural and the donor is willing to be there for us. Yeah, we're all really close so it would be nice if he could be involved directly. Amazing. That sounds like a soul of the blind. Now Laura, are you taking any supplements like folic acid or making any lifestyle changes in the preparation for the process? Yes, I've been taking folic acid for a few months now and I've kept down caffeine and also I'm trying to eat a few. Yeah, she's been really good about it. We've both been trying to improve our diets together. Oh, that's excellent. A balanced diet and staying active can be very helpful indeed. Mary, I'm glad to hear that you are both supporting each other through this. Do you have any other concerns or questions about the process? Just one. Yeah. How soon can we get started? We're ready to move forward as soon as possible. Well, your infuse hasn't is really great. Once we get the initial test results back and your donor is secreined, we can plan the timing for the insemination or the IOI. Typically, it doesn't take long to set up. We'll get the ball rolling right away. That's great news. We were worried it would be a long way. Yes, thank you so much Dr. Burnett. This feels like I feel a lot more confident about everything. Thank you. You're a very welcome guys and good luck. Thank you.\n"
          ]
        }
      ],
      "source": [
        "# Load the Whisper model\n",
        "import whisper\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "# Transcribe the audio\n",
        "result = model.transcribe(\"Interview_test.wav\")\n",
        "print(result[\"text\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
            "found 0 physical cores < 1\n",
            "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
            "  warnings.warn(\n",
            "  File \"c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
            "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n"
          ]
        }
      ],
      "source": [
        "seg = extract_speakers(model,'Interview_test.wav')\n",
        "write_segments(seg, 'transcript.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\User\\.cache\\torch\\pyannote\\models--pyannote--segmentation\\snapshots\\c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b\\pytorch_model.bin`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.5.0+cpu. Bad things might happen unless you revert torch to 1.x.\n"
          ]
        }
      ],
      "source": [
        "from pyannote.audio import Pipeline\n",
        "\n",
        "# Charger la pipeline pré-entraînée pour la diarisation\n",
        "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\",use_auth_token= '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')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m diarization \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmono.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyannote\\audio\\core\\pipeline.py:327\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, file, **kwargs)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessors\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    325\u001b[0m     file \u001b[38;5;241m=\u001b[39m ProtocolFile(file, lazy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessors)\n\u001b[1;32m--> 327\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyannote\\audio\\pipelines\\speaker_diarization.py:523\u001b[0m, in \u001b[0;36mSpeakerDiarization.apply\u001b[1;34m(self, file, num_speakers, min_speakers, max_speakers, return_embeddings, hook)\u001b[0m\n\u001b[0;32m    520\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 523\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbinarized_segmentations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_overlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_exclude_overlap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    529\u001b[0m     hook(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m, embeddings)\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;66;03m#   shape: (num_chunks, local_num_speakers, dimension)\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyannote\\audio\\pipelines\\speaker_diarization.py:348\u001b[0m, in \u001b[0;36mSpeakerDiarization.get_embeddings\u001b[1;34m(self, file, binary_segmentations, exclude_overlap, hook)\u001b[0m\n\u001b[0;32m    345\u001b[0m mask_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mvstack(masks)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# (batch_size, num_frames) torch.Tensor\u001b[39;00m\n\u001b[1;32m--> 348\u001b[0m embedding_batch: np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwaveform_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_batch\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# (batch_size, dimension) np.ndarray\u001b[39;00m\n\u001b[0;32m    353\u001b[0m embedding_batches\u001b[38;5;241m.\u001b[39mappend(embedding_batch)\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyannote\\audio\\pipelines\\speaker_verification.py:374\u001b[0m, in \u001b[0;36mSpeechBrainPretrainedSpeakerEmbedding.__call__\u001b[1;34m(self, waveforms, masks)\u001b[0m\n\u001b[0;32m    370\u001b[0m wav_lens \u001b[38;5;241m=\u001b[39m wav_lens \u001b[38;5;241m/\u001b[39m max_len\n\u001b[0;32m    371\u001b[0m wav_lens[too_short] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m    373\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 374\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwav_lens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwav_lens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;241m.\u001b[39msqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    378\u001b[0m )\n\u001b[0;32m    380\u001b[0m embeddings[too_short\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\speechbrain\\inference\\classifiers.py:112\u001b[0m, in \u001b[0;36mEncoderClassifier.encode_batch\u001b[1;34m(self, wavs, wav_lens, normalize)\u001b[0m\n\u001b[0;32m    110\u001b[0m feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmods\u001b[38;5;241m.\u001b[39mcompute_features(wavs)\n\u001b[0;32m    111\u001b[0m feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmods\u001b[38;5;241m.\u001b[39mmean_var_norm(feats, wav_lens)\n\u001b[1;32m--> 112\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwav_lens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normalize:\n\u001b[0;32m    114\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams\u001b[38;5;241m.\u001b[39mmean_var_norm_emb(\n\u001b[0;32m    115\u001b[0m         embeddings, torch\u001b[38;5;241m.\u001b[39mones(embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    116\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\speechbrain\\lobes\\models\\ECAPA_TDNN.py:517\u001b[0m, in \u001b[0;36mECAPA_TDNN.forward\u001b[1;34m(self, x, lengths)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 517\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    519\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer(x)\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\speechbrain\\lobes\\models\\ECAPA_TDNN.py:374\u001b[0m, in \u001b[0;36mSERes2NetBlock.forward\u001b[1;34m(self, x, lengths)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshortcut:\n\u001b[0;32m    372\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshortcut(x)\n\u001b[1;32m--> 374\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtdnn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    375\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres2net_block(x)\n\u001b[0;32m    376\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtdnn2(x)\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\speechbrain\\lobes\\models\\ECAPA_TDNN.py:81\u001b[0m, in \u001b[0;36mTDNNBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     80\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Processes the input tensor x and returns an output tensor.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\speechbrain\\nnet\\CNN.py:455\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    450\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPadding must be \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcausal\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    452\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding\n\u001b[0;32m    453\u001b[0m     )\n\u001b[1;32m--> 455\u001b[0m wx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munsqueeze:\n\u001b[0;32m    458\u001b[0m     wx \u001b[38;5;241m=\u001b[39m wx\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:375\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:370\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[0;32m    360\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    361\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    369\u001b[0m     )\n\u001b[1;32m--> 370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "diarization = pipeline(\"mono.wav\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[61], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m segments\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Utilisation\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m segments \u001b[38;5;241m=\u001b[39m \u001b[43mextract_speakers_pyannote\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mInterview_test_converted.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Afficher les segments avec les locuteurs\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seg \u001b[38;5;129;01min\u001b[39;00m segments:\n",
            "Cell \u001b[1;32mIn[61], line 4\u001b[0m, in \u001b[0;36mextract_speakers_pyannote\u001b[1;34m(audio_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_speakers_pyannote\u001b[39m(audio_path):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Effectuer la diarisation en utilisant pyannote\"\"\"\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     diarization \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     segments \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m turn, _, speaker \u001b[38;5;129;01min\u001b[39;00m diarization\u001b[38;5;241m.\u001b[39mitertracks(yield_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyannote\\audio\\core\\pipeline.py:327\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, file, **kwargs)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessors\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    325\u001b[0m     file \u001b[38;5;241m=\u001b[39m ProtocolFile(file, lazy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessors)\n\u001b[1;32m--> 327\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyannote\\audio\\pipelines\\speaker_diarization.py:523\u001b[0m, in \u001b[0;36mSpeakerDiarization.apply\u001b[1;34m(self, file, num_speakers, min_speakers, max_speakers, return_embeddings, hook)\u001b[0m\n\u001b[0;32m    520\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 523\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbinarized_segmentations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_overlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_exclude_overlap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    529\u001b[0m     hook(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m, embeddings)\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;66;03m#   shape: (num_chunks, local_num_speakers, dimension)\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyannote\\audio\\pipelines\\speaker_diarization.py:348\u001b[0m, in \u001b[0;36mSpeakerDiarization.get_embeddings\u001b[1;34m(self, file, binary_segmentations, exclude_overlap, hook)\u001b[0m\n\u001b[0;32m    345\u001b[0m mask_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mvstack(masks)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# (batch_size, num_frames) torch.Tensor\u001b[39;00m\n\u001b[1;32m--> 348\u001b[0m embedding_batch: np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwaveform_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_batch\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# (batch_size, dimension) np.ndarray\u001b[39;00m\n\u001b[0;32m    353\u001b[0m embedding_batches\u001b[38;5;241m.\u001b[39mappend(embedding_batch)\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyannote\\audio\\pipelines\\speaker_verification.py:374\u001b[0m, in \u001b[0;36mSpeechBrainPretrainedSpeakerEmbedding.__call__\u001b[1;34m(self, waveforms, masks)\u001b[0m\n\u001b[0;32m    370\u001b[0m wav_lens \u001b[38;5;241m=\u001b[39m wav_lens \u001b[38;5;241m/\u001b[39m max_len\n\u001b[0;32m    371\u001b[0m wav_lens[too_short] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m    373\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 374\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwav_lens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwav_lens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;241m.\u001b[39msqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    378\u001b[0m )\n\u001b[0;32m    380\u001b[0m embeddings[too_short\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\speechbrain\\inference\\classifiers.py:112\u001b[0m, in \u001b[0;36mEncoderClassifier.encode_batch\u001b[1;34m(self, wavs, wav_lens, normalize)\u001b[0m\n\u001b[0;32m    110\u001b[0m feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmods\u001b[38;5;241m.\u001b[39mcompute_features(wavs)\n\u001b[0;32m    111\u001b[0m feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmods\u001b[38;5;241m.\u001b[39mmean_var_norm(feats, wav_lens)\n\u001b[1;32m--> 112\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwav_lens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normalize:\n\u001b[0;32m    114\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams\u001b[38;5;241m.\u001b[39mmean_var_norm_emb(\n\u001b[0;32m    115\u001b[0m         embeddings, torch\u001b[38;5;241m.\u001b[39mones(embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    116\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\speechbrain\\lobes\\models\\ECAPA_TDNN.py:527\u001b[0m, in \u001b[0;36mECAPA_TDNN.forward\u001b[1;34m(self, x, lengths)\u001b[0m\n\u001b[0;32m    524\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmfa(x)\n\u001b[0;32m    526\u001b[0m \u001b[38;5;66;03m# Attentive Statistical Pooling\u001b[39;00m\n\u001b[1;32m--> 527\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    528\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masp_bn(x)\n\u001b[0;32m    530\u001b[0m \u001b[38;5;66;03m# Final linear transformation\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\speechbrain\\lobes\\models\\ECAPA_TDNN.py:280\u001b[0m, in \u001b[0;36mAttentiveStatisticsPooling.forward\u001b[1;34m(self, x, lengths)\u001b[0m\n\u001b[0;32m    277\u001b[0m     attn \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    279\u001b[0m \u001b[38;5;66;03m# Apply layers\u001b[39;00m\n\u001b[1;32m--> 280\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtdnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m    282\u001b[0m \u001b[38;5;66;03m# Filter out zero-paddings\u001b[39;00m\n\u001b[0;32m    283\u001b[0m attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39mmasked_fill(mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\speechbrain\\lobes\\models\\ECAPA_TDNN.py:81\u001b[0m, in \u001b[0;36mTDNNBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     80\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Processes the input tensor x and returns an output tensor.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\speechbrain\\nnet\\CNN.py:455\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    450\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPadding must be \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcausal\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    452\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding\n\u001b[0;32m    453\u001b[0m     )\n\u001b[1;32m--> 455\u001b[0m wx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munsqueeze:\n\u001b[0;32m    458\u001b[0m     wx \u001b[38;5;241m=\u001b[39m wx\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:375\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:370\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[0;32m    360\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    361\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    369\u001b[0m     )\n\u001b[1;32m--> 370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "def extract_speakers_pyannote(audio_path):\n",
        "    \"\"\"Effectuer la diarisation en utilisant pyannote\"\"\"\n",
        "    \n",
        "\n",
        "    segments = []\n",
        "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
        "        print({\n",
        "            \"start\": turn.start,\n",
        "            \"end\": turn.end,\n",
        "            \"speaker\": speaker\n",
        "        })\n",
        "        segments.append({\n",
        "            \"start\": turn.start,\n",
        "            \"end\": turn.end,\n",
        "            \"speaker\": speaker\n",
        "        })\n",
        "        \n",
        "    \n",
        "    return segments\n",
        "\n",
        "# Utilisation\n",
        "segments = extract_speakers_pyannote(\"Interview_test_converted.wav\")\n",
        "# Afficher les segments avec les locuteurs\n",
        "for seg in segments:\n",
        "    print(f\"{seg['speaker']} : {seg['start']} - {seg['end']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "changement:\n",
        "- \n",
        "-\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
